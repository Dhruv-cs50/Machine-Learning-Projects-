{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sBJ7eiTYkB80"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9i6nZVy7kh_V"},"source":["## Assignment 1\n","## Introduction and Linear Regression\n","\n","\n","**(Due Sunday 2/02/2025 before 11:59pm)**"]},{"cell_type":"markdown","metadata":{"id":"QprsA7J5lwpt"},"source":["# 1- Reading\n","\n","[Lecture Notes by Andrew Ng](https://cs229.stanford.edu/lectures-spring2022/main_notes.pdf) Lesson 1 Linear Regression (1.1 - 1.3)"]},{"cell_type":"markdown","metadata":{"id":"hKDUkdQCoAMz"},"source":["# 2- Answer the Following Questions.\n","*Return your filled notebook **and** a pdf version of it.*"]},{"cell_type":"markdown","metadata":{"id":"K6voaEAenipU"},"source":["**Question 1** (10%):  \n","In your own words define what a linear regression is. What do you need to create a linear regression?"]},{"cell_type":"markdown","metadata":{"id":"LkcUoBDtqCKL"},"source":["Linear regression is a statistical method that explains and examines the relationship between an independent variable and one or more independent variables by approximating a linear equation to the data. The main aim of linear regression is to forecast the value of an independent variable based on the value of the independent variables. \n","\n","In order to create a linear regression model, the following elements are required:\n","\n","1. **Data**: You need a dataset in which you have a dependent variable, the variable you are trying to predict or explain, and one or more independent variables, the predictors.\n","\n","2. **Adherence to Assumptions**: The dataset needs to meet some important assumptions for linear regression to work effectively:\n","   - **Linearity**: There should be a natural linear relationship between the independent and dependent variables.\n","   - **Independence**: The observations are independent of each other.\n","   - **Homoscedasticity**: The residuals (or prediction error) should have the same variance for all levels of the independent variables.\n","   - **Normality of Errors**: The residuals should be normally distributed.\n","\n","3. **Statistical Tools or Software**: Statistics software such as Python (with libraries like scikit-learn or statsmodels), R, or even spreadsheet software like Excel is needed to compute the regression. These are utilized to fit the linear model to data and extract the regression coefficients.\n","\n","4. **Regression Equation**: The model is typically written in the form of a regression equation:\n","\n","   Y= Œ≤0 + Œ≤1 X 1+ ... +Œ≤n X +œµ\n","\n","Here,  Y  is the dependent variable, beta_0 is the intercept of the equation, beta_1, dots, beta_n  are coefficients that represent the contribution of each of the independent variables  X_1, dots, X_n , and epsilon  is the error term that represents any variation that is not explained by the independent variables.\n","\n","With these components, a linear regression model can be built to predict, analyze the relationship among variables, and perform different analyses in order to get significant conclusions out of data.\n"]},{"cell_type":"markdown","metadata":{"id":"wOjZgCgeqHOB"},"source":["**Question 2:** (30%) What loss function is typically used for Linear Regression?  Write pseudo-code (or Python code if you prefer)  that describes an implemention the loss function J(x, y, theta), where x is a matrix of samples, y is a vector of corresponding targets, and theta are the parameters of the model"]},{"cell_type":"markdown","metadata":{},"source":["For linear regression, the typical loss function used is the Mean Squared Error (MSE). This function calculates the average of the squares of the differences between the predicted and actual values. The MSE effectively measures the quality of a linear regression model, penalizing larger errors more severely than smaller ones, which makes it quite useful for fitting a model."]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.1666666666666667\n"]}],"source":["import numpy as np\n","\n","def mse_loss(x, y, theta):\n","    m = len(y)  # Number of samples\n","    predictions = x.dot(theta)  # Compute the predictions\n","    errors = predictions - y  # Calculate the errors\n","    squared_errors = errors ** 2  # Square the errors\n","    mse = np.mean(squared_errors)  # Calculate the mean of squared errors\n","    return mse\n","\n","x = np.array([[1, 2], [1, 3], [1, 4]])  # Example feature matrix (with a column of ones for the intercept)\n","y = np.array([5, 6, 7])  # Example target vector\n","theta = np.array([0.5, 1.5])  # Example parameter vector\n","print(mse_loss(x, y, theta))"]},{"cell_type":"markdown","metadata":{},"source":["This function demonstrates how to implement the MSE loss in a practical scenario using NumPy, which is a common Python library for numerical computations. The function accepts the feature matrix ùë•, the target vector y, and the parameter vector Œ∏, and it returns the MSE loss calculated using these inputs."]},{"cell_type":"markdown","metadata":{"id":"BPTpfJMysLpR"},"source":["**Question 3** (30%): Write Pseudocode (or Python code if you prefer) that implements the LMS gradient descent algorithm."]},{"cell_type":"markdown","metadata":{"id":"y_jsz59jPFr5"},"source":["The Least Mean Squares (LMS) algorithm, often used for linear regression, employs a gradient descent method to minimize the loss function, typically Mean Squared Error (MSE). This iterative approach adjusts the parameters Œ∏ incrementally based on the gradient of the loss function with respect to these parameters."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 0: Loss = 28.516666666666666\n","Iteration 100: Loss = 0.3547397435758479\n","Iteration 200: Loss = 0.31281241974461954\n","Iteration 300: Loss = 0.2758405652092049\n","Iteration 400: Loss = 0.24323847971590165\n","Iteration 500: Loss = 0.21448969251361105\n","Iteration 600: Loss = 0.18913877544505872\n","Iteration 700: Loss = 0.16678412821439584\n","Iteration 800: Loss = 0.14707161637681365\n","Iteration 900: Loss = 0.12968896126544993\n","Optimized Parameters: [1.71264897 1.40213818]\n"]}],"source":["import numpy as np\n","\n","def lms_gradient_descent(x, y, theta, alpha, num_iterations):\n","    \n","    m = len(y)  # Number of samples\n","    for i in range(num_iterations):\n","        predictions = x.dot(theta)  # Calculate predictions\n","        errors = predictions - y  # Compute errors\n","        gradient = x.T.dot(errors) / m  # Calculate gradient\n","        theta -= alpha * gradient  # Update parameters\n","        # Optional: Print loss every some steps or check for convergence\n","        if i % 100 == 0:\n","            loss = np.mean(errors ** 2)\n","            print(f\"Iteration {i}: Loss = {loss}\")\n","    return theta\n","\n","x = np.array([[1, 2], [1, 3], [1, 4]])  # Example feature matrix (with a column of ones for the intercept)\n","y = np.array([5, 6, 7])  # Example target vector\n","theta = np.array([0.1, 0.2])  # Initial parameter vector\n","alpha = 0.01  # Learning rate\n","num_iterations = 1000  # Number of iterations\n","optimized_theta = lms_gradient_descent(x, y, theta, alpha, num_iterations)\n","print(f\"Optimized Parameters: {optimized_theta}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["This function sets up the parameters and performs a specified number of iterations of gradient descent. Each iteration computes the predictions, determines the gradient of the error, and updates the parameters based on this gradient. The learning rate \n","ùõº\n","Œ± controls how large each step is during the parameter update. The number of iterations controls how long the algorithm runs before it stops. Optional outputs can include periodic loss values or checks for convergence, which can be useful for monitoring the progress of the algorithm."]},{"cell_type":"markdown","metadata":{"id":"t9Rwq5FpvZ_Q"},"source":["**Question 4** 20%): What is the role of the learning rate in the LMS updates? What happens if its value is too small? or when it is too large?"]},{"cell_type":"markdown","metadata":{"id":"PFdv-ys2vp8v"},"source":["Answer: The learning rate is another important hyperparameter in LMS gradient descent; it is actually supposed to guarantee the convergence of your model to the most optimal set of parameters. It means this is a scalar value (normally represented as alpha  that influences how large a step the update of parameters will make on each iteration of gradient descent.\n","\n","### Learning Rate\n","The learning rate alpha  controls the magnitude of a step that the parameters theta will take toward changing in the opposite direction of the calculated gradient of the loss function. If the learning rate is chosen adequately, this will allow for efficient convergence toward a minimum of the loss function, therefore finding the best fitting parameters for the model.\n","\n","### Consequences of Too Small Learning Rate\n","1. **Slow Convergence**: When using a too-small fraction of learning rate, each step towards minimizing the loss is extremely tiny; hence, a very slow convergence process that could be computationally expensive and really time-consuming if many iterations have to be waited on for getting an optimal result.\n","2. **Risk of Stalling**: Too small learning rate may lead to stalling in the process of training due to too tiny updates that would be unable to induce significant alteration in the model parameters, sometimes accompanied by round-off errors, or computational inaccuracies.\n","\n","### Consequences of a Too Large Learning Rate\n","1. **Overshooting the Minimum**: If the learning rate is too large, then the update in the parameters might also become very large to such an extent that the algorithm may overshoot the minimum of the loss. The result might be divergence from, instead of convergence to, the optimum.\n","2. **Oscillations**: With a high learning rate, the algorithm could keep overshooting the minimum on each iteration to the next side and never converge, or worse, updates could get ever larger as the gradient keeps flip-flopping in direction.\n","3. **Instability**: If the learning rate is set too high, numerical stability may be a problem during computation, and one may end up with non-makingsensical results.\n"," \n","### Optimal Learning Rate\n","In general, the best learning rate often represents a compromise between these extremes: large enough to ensure reasonable convergence times, but not so large as to result in overshooting or instability. This usually involves some experimentation with learning rate choice and is often purely heuristic, sometimes using trial-and-error approaches, or more systematic ones such as grid search, learning rate schedules, or adaptive learning rate methods like Adam or RMSprop that change the learning rate dynamically based on recent updates.\n"]},{"cell_type":"markdown","metadata":{"id":"z-zh98UJwZWY"},"source":["**Question 5** (10%): What are the strengths and limitations of both the Least Square Method and the Normal Equation method (pseudo-inverse)?  When should you use one or the other?"]},{"cell_type":"markdown","metadata":{"id":"kfX3IZhDwzTi"},"source":["**Answer:**\n","\n","### Least Squares Method (Gradient Descent)\n","**Strengths:**\n","1. **Scalability**: Handles large datasets efficiently.\n","2. **Flexibility**: Adapts to various types of problems through adjustable hyperparameters.\n","3. **Online Learning**: Updates model parameters as new data arrives.\n","\n","**Limitations:**\n","1. **Convergence Issues**: Requires careful tuning of learning rate and iterations.\n","2. **Computationally Intensive**: Each iteration involves computations over the entire dataset.\n","\n","### Normal Equation Method (Pseudo-inverse)\n","**Strengths:**\n","1. **Simplicity**: Provides a direct solution without iterative optimization.\n","2. **Exact Solution**: Achieves the exact solution in one calculation if the matrix is invertible.\n","\n","**Limitations:**\n","1. **Computational Complexity**: Computation is expensive and unstable with many features.\n","2. **Memory Intensive**: Requires significant memory to compute, problematic with large datasets.\n","\n","### When to Use Each Method\n","- **Use Gradient Descent**: Recommended for large datasets or when computational resources are a concern. Ideal for scenarios requiring model updates with new data.\n","- **Use Normal Equation**: Best for smaller datasets or when the number of features is manageable. Suitable when a precise, direct solution is needed without computational limitations.\n","\n","The choice between these methods typically depends on dataset size, computational capabilities, and the dynamic nature of the data being modeled."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyM0vCt3prndik2bSdnoNSEk","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.1"}},"nbformat":4,"nbformat_minor":0}
